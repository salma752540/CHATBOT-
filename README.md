<<<<<<< HEAD
# ImplÃ©mentation d'un Chatbot BasÃ© sur LangChain


### Introduction
Ce document prÃ©sente le rapport technique pour lâ€™implÃ©mentation dâ€™un chatbot utilisant la bibliothÃ¨que LangChain. Le chatbot est conÃ§u pour rÃ©pondre Ã  des questions en exploitant des donnÃ©es contextuelles stockÃ©es dans un vecteurstore FAISS et un modÃ¨le prÃ©entraÃ®nÃ© Ã  base de langage. Lâ€™objectif principal est de fournir des rÃ©ponses prÃ©cises et concises, tout en assurant une gestion adaptÃ©e des Ã©ventuelles inconnues.


### Architecture du Chatbot

#### 1. **Modules UtilisÃ©s**
- **LangChain Community** : Pour le chargement des documents (Ã  partir de fichiers PDF) et la gestion des embeddings.
- **FAISS** : SystÃ¨me de stockage vectoriel pour les recherches efficaces de similaritÃ©.
- **CTransformers** : Pour intÃ©grer le modÃ¨le de langage Llama 2.
- **Chainlit** : Interface utilisateur interactive permettant la gestion des Ã©changes avec les utilisateurs.

#### 2. **Pipeline Fonctionnel**
Le chatbot suit les Ã©tapes suivantes :
1. **Chargement des Embeddings** : Utilisation de Â« sentence-transformers/all-MiniLM-L6-v2 Â» pour reprÃ©senter les donnÃ©es en embeddings vectoriels.
2. **Chargement du ModÃ¨le** : ModÃ¨le Llama 2 prÃ©entraÃ®nÃ© chargÃ© via la bibliothÃ¨que CTransformers.
3. **Initialisation du Prompt** : ModÃ¨le de prompt personnalisÃ© pour fournir des rÃ©ponses optimales et Ã©viter des rÃ©ponses non fiables.
4. **Recherche Contextuelle** : Utilisation de FAISS comme vecteurstore pour rechercher les contextes les plus pertinents.
5. **Interaction avec lâ€™Utilisateur** :
   - Gestion asynchrone via Chainlit.
   - Formatage et prÃ©sentation des rÃ©ponses, incluant les sources si disponibles.


### FonctionnalitÃ©s DÃ©taillÃ©es

#### 1. **Gestion du Prompt PersonnalisÃ©**
Le prompt est conÃ§u pour limiter les rÃ©ponses non fiables. Voici le modÃ¨le utilisÃ© :
```text
Use the following pieces of information to answer the user's question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}
Question: {question}

Only return the helpful answer below and nothing else.
Helpful answer:
```

#### 2. **IntÃ©gration de FAISS**
FAISS permet dâ€™indexer et de rechercher efficacement les documents Ã  partir de leurs embeddings vectoriels. Dans le code, la base FAISS est initialisÃ©e avec le chemin suivant :
```
DB_FAISS_PATH = 'vectorstores/db_faiss'
```

#### 3. **ModÃ¨le de Langage**
Le modÃ¨le **Llama 2-7B** est utilisÃ©, avec les paramÃ¨tres suivants :
- **Nombre maximal de tokens** : 512.
- **TempÃ©rature** : 0.5 (pour Ã©quilibrer la diversitÃ© et la prÃ©cision des rÃ©ponses).

#### 4. **Gestion des RÃ©ponses et des Sources**
Le chatbot retourne les rÃ©ponses au format suivant :
- **Question** : Rappel de la question posÃ©e.
- **RÃ©ponse** : RÃ©ponse gÃ©nÃ©rÃ©e par le modÃ¨le.
- **Source** : Liste des documents consultÃ©s avec les numÃ©ros de page.
En cas dâ€™absence de sources, un message spÃ©cifique est prÃ©sentÃ©.

#### 5. **FonctionnalitÃ©s Asynchrones**
Lâ€™interaction utilisateur est gÃ©rÃ©e de maniÃ¨re asynchrone pour garantir une fluiditÃ© optimale via Chainlit. Cela inclut :
- DÃ©tection des rÃ©ponses rÃ©pÃ©tÃ©es.
- Mise Ã  jour des messages sans interrompre la session utilisateur.

### **FlexibilitÃ© et RÃ©utilisabilitÃ© du Chatbot**

Notre chatbot est conÃ§u pour Ãªtre gÃ©nÃ©rique et adaptable grÃ¢ce Ã  sa structure modulaire. Voici les points qui mettent en avant cette rÃ©utilisabilitÃ© :

- **IndÃ©pendance des donnÃ©es :**

Le modÃ¨le est entraÃ®nÃ© Ã  partir d'embeddings vectoriels gÃ©nÃ©rÃ©s Ã  partir des documents fournis (PDF ou autres formats).
Ces embeddings capturent la signification et le contexte des donnÃ©es, permettant au chatbot de s'adapter Ã  tout type de contenu.
- **ModularitÃ© de la Solution :**

* Chargement des donnÃ©es : Vous pouvez remplacer les documents mÃ©dicaux actuels par des donnÃ©es d'autres domaines (par exemple, droit, finance, Ã©ducation).
* FAISS et Qdrant : La base de donnÃ©es vectorielle peut Ãªtre recrÃ©Ã©e pour chaque ensemble de donnÃ©es, garantissant une recherche rapide et pertinente dans n'importe quel domaine.
ModÃ¨le Llama 2 PrÃ©entraÃ®nÃ© :

Le modÃ¨le Llama 2 utilisÃ© est dÃ©jÃ  gÃ©nÃ©raliste. En modifiant simplement le contexte fourni dans les donnÃ©es, il peut rÃ©pondre Ã  des questions sur divers sujets.
- **Exemple de RÃ©utilisation :**

Aujourdâ€™hui, notre chatbot rÃ©pond Ã  des questions mÃ©dicales sur le cancer du cerveau.
Demain, en chargeant des donnÃ©es juridiques (par exemple, des lois et des rÃ¨glements), il pourrait devenir un assistant juridique pour aider les utilisateurs Ã  interprÃ©ter les lois.
- **FacilitÃ© de Mise en Å’uvre :**

Il suffit de :
* Collecter les nouveaux documents pertinents.
* GÃ©nÃ©rer des embeddings Ã  partir de ces documents.
* RÃ©initialiser la base de donnÃ©es vectorielle FAISS/Qdrant.
* DÃ©ployer le chatbot avec ces nouvelles donnÃ©es.
### **Avantages pour les Utilisateurs :**


Ã‰conomique : Un seul framework pour plusieurs domaines.

Rapide : Adaptation quasi-instantanÃ©e aux nouvelles donnÃ©es.

Efficace : RÃ©pond aux besoins spÃ©cifiques sans avoir Ã  recrÃ©er un nouveau modÃ¨le.


---

### Conclusion
Le chatbot implÃ©mentÃ© est une solution robuste pour gÃ©rer des requÃªtes basÃ©es sur un contexte documentaire. En exploitant LangChain, FAISS et un modÃ¨le Llama 2, il fournit des rÃ©ponses pertinentes tout en maintenant une expÃ©rience utilisateur adaptÃ©e. Par exemple, ce chatbot pourrait Ãªtre utilisÃ© dans le domaine mÃ©dical pour rÃ©pondre Ã  des questions spÃ©cifiques Ã  partir de publications scientifiques, ou dans les entreprises pour fournir une assistance interne basÃ©e sur leurs bases de donnÃ©es documentaires. Avec quelques amÃ©liorations, cette solution pourrait Ãªtre dÃ©ployÃ©e Ã  grande Ã©chelle pour divers cas dâ€™utilisation.

=======


# Llama2 Medical Bot

The Llama2 Medical Bot is a powerful tool designed to provide medical information by answering user queries using state-of-the-art language models and vector stores. This README will guide you through the setup and usage of the Llama2 Medical Bot.

## Table of Contents

- [Introduction](#langchain-medical-bot)
- [Table of Contents](#table-of-contents)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Prerequisites

Before you can start using the Llama2 Medical Bot, make sure you have the following prerequisites installed on your system:

- Python 3.6 or higher
- Required Python packages (you can install them using pip):
    - langchain
    - chainlit
    - sentence-transformers
    - faiss
    - PyPDF2 (for PDF document loading)

## Installation

1. Clone this repository to your local machine.

    ```bash
    git clone https://github.com/your-username/langchain-medical-bot.git
    cd langchain-medical-bot
    ```

2. Create a Python virtual environment (optional but recommended):

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use: venv\Scripts\activate
    ```

3. Install the required Python packages:

    ```bash
    pip install -r requirements.txt
    ```

4. Download the required language models and data. Please refer to the Langchain documentation for specific instructions on how to download and set up the language model and vector store.

5. Set up the necessary paths and configurations in your project, including the `DB_FAISS_PATH` variable and other configurations as per your needs.

## Getting Started

To get started with the Llama2 Medical Bot, you need to:

1. Set up your environment and install the required packages as described in the Installation section.

2. Configure your project by updating the `DB_FAISS_PATH` variable and any other custom configurations in the code.

3. Prepare the language model and data as per the Langchain documentation.

4. Start the bot by running the provided Python script or integrating it into your application.

## Usage

The Llama2 Medical Bot can be used for answering medical-related queries. To use the bot, you can follow these steps:

1. Start the bot by running your application or using the provided Python script.

2. Send a medical-related query to the bot.

3. The bot will provide a response based on the information available in its database.

4. If sources are found, they will be provided alongside the answer.

5. The bot can be customized to return specific information based on the query and context provided.

## Contributing

Contributions to the Llama2 Medical Bot are welcome! If you'd like to contribute to the project, please follow these steps:

1. Fork the repository to your own GitHub account.

2. Create a new branch for your feature or bug fix.

3. Make your changes and ensure that the code passes all tests.

4. Create a pull request to the main repository, explaining your changes and improvements.

5. Your pull request will be reviewed, and if approved, it will be merged into the main codebase.

## License

This project is licensed under the MIT License.

---

For more information on how to use, configure, and extend the Llama2 Medical Bot, please refer to the Langchain documentation or contact the project maintainers.

Happy coding with Llama2 Medical Bot! ðŸš€
>>>>>>> b37f3012 (Premier commit : ajout des fichiers)
